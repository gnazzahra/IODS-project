# 4: Clustering and Classification
```{r}
date()
```
*This week I am learning about clustering and classification. Now I will do an analysis excercise.*


```{r}
#reading the data
library(MASS)
data("Boston")
dim(Boston)
#printing out the names of the variables
str(Boston)
```

- Here, we have a data collected from 506 observations with 14 variables. 
- This data describes housing values in suburbs of Boston.
- The variable medv stands for the median value of ownner-occupied homes in $1000s.


```{r}
library(tidyr)
library(corrplot)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

#summary
summary(Boston)

```

- Median value has strong negative correlation to lower status of the population and strong positive correlation to average number of rooms per dwelling.
- Aside from that, median value also has negative correlation to pupil-teacher ratio in town, full-value property-tax rate per $10,000, index of accessibility to radial highways, proportion of owner-occupied units built prior to 1940, nitrogen oxides concentration (parts per 10 million), proportion of non-retail business acres per town, and per capita crime rate by town.
- Median value also has positive correlation to proportion of residential land zoned for lots over 25,000 sq.ft, Charles River dummy variable (= 1 if tract bounds river; 0 otherwise), weighted mean of distances to five Boston employment centres, and proportion of blacks by town.
- The average of house median value is $22,530, while the lowest median value is $5,000 and the highest median value is $50,000.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

- The data has now been standardized. Now, the mean of all variables are 0. 

```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label= c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

- The variable crime is now categorical with 4 categories according to the crime rate value quantiles. 

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

- The data has now been divided into train (80%) set and test (20%) set.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)

```

- Now, we have fitted the linear discriminant analysis on the train set. The categorical crime rate was used as the target variable while all other variables are predictor variables.
- The variable rad seems to be the most influential linear separator for the clusters.

```{r}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
- High category has the highest correct prediction rate; only 1 observation is predicted incorrectly.
- Med-high category also has a quite high correct prediction rate. Out of 16 observations, 13 are predicted correctly.
- Med-low category has the most incorrect predictions; out of 31 observations, only 13 are predicted correctly.
- The prediction rate of low category is better, out of 28 observations, 20 are predicted correctly.

```{r}
# reload Boston data
data('Boston')

# center and standardize variables
boston_scaled <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# k-means clustering: determining the k
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

- Based on the plot above, it seems like 2, 6, or 8 centers is the most appropriate.I will try using 2 centers first.

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the dataset with clusters
pairs(boston_scaled[,1:4], col = km$cluster)
pairs(boston_scaled[,5:7], col = km$cluster)
pairs(boston_scaled[,8:10], col = km$cluster)
pairs(boston_scaled[,11:14], col = km$cluster)
```

- The data is now divided into 2 categories. 

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 6)

# plot the dataset with clusters
pairs(boston_scaled[,1:4], col = km$cluster)
pairs(boston_scaled[,5:7], col = km$cluster)
pairs(boston_scaled[,8:10], col = km$cluster)
pairs(boston_scaled[,11:14], col = km$cluster)
```

```{r}
library(MASS)
data(Boston)
boston_bonus <- scale(Boston)
summary(boston_bonus)
boston_bonus <- as.data.frame(boston_bonus)

# k-means clustering
km <-kmeans(boston_bonus, centers = 6)

# linear discriminant analysis
lda.fit <- lda(km$cluster ~ ., data = boston_bonus)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```

- Variable chas seems to be the most influential separator for the clusters.

